{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "digit_recognizer1.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNsYU41Yu6JLmoJDO1i0iPb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ikikika/data_science/blob/digit_recognizer-kaggle/digit_recognizer/digit_recognizer1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Eqo9A99mv9u",
        "colab_type": "text"
      },
      "source": [
        "# Digit Recognizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bMCAVUPVljhn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "d6ff5d83-7898-4077-ee2f-4bd546438f9b"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "\n",
        "np.random.seed(2)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import itertools\n",
        "\n",
        "from keras.utils.np_utils import to_categorical # convert to one-hot-encoding\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\n",
        "from keras.optimizers import RMSprop\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "\n",
        "sns.set( style='white', context='notebook', palette='deep' )"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n",
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KpuT_dgSwmaM",
        "colab_type": "text"
      },
      "source": [
        "## 1. Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8NrHwDbny2u",
        "colab_type": "text"
      },
      "source": [
        "### Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CqFcvyukn0r6",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "426a9057-e2df-4d88-8f7f-35e1c6fe436d"
      },
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-a7170973-dbd5-4130-8ce9-397355de8069\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-a7170973-dbd5-4130-8ce9-397355de8069\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kaggle.json': b'{\"username\":\"ikikika\",\"key\":\"58b37dbfd627308d2ee21d09aaff608d\"}'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Yg7hShwoxPc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "54cb1ac5-377f-4c9a-8675-6ea99a028f1d"
      },
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "!ls ~/.kaggle"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "kaggle.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fV1RQD4FoOyp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "9b2811af-37a0-4803-aaf5-f6dd0ba43f62"
      },
      "source": [
        "!kaggle competitions list"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.6 / client 1.5.4)\n",
            "ref                                            deadline             category            reward  teamCount  userHasEntered  \n",
            "---------------------------------------------  -------------------  ---------------  ---------  ---------  --------------  \n",
            "tpu-getting-started                            2030-06-03 23:59:00  Getting Started      Kudos        188           False  \n",
            "digit-recognizer                               2030-01-01 00:00:00  Getting Started  Knowledge       2974           False  \n",
            "titanic                                        2030-01-01 00:00:00  Getting Started  Knowledge      22488            True  \n",
            "house-prices-advanced-regression-techniques    2030-01-01 00:00:00  Getting Started  Knowledge       5075            True  \n",
            "connectx                                       2030-01-01 00:00:00  Getting Started  Knowledge        764           False  \n",
            "nlp-getting-started                            2030-01-01 00:00:00  Getting Started      Kudos       1506           False  \n",
            "competitive-data-science-predict-future-sales  2020-12-31 23:59:00  Playground           Kudos       7816           False  \n",
            "osic-pulmonary-fibrosis-progression            2020-10-06 23:59:00  Featured           $55,000        250           False  \n",
            "halite                                         2020-09-15 23:59:00  Featured              Swag        701           False  \n",
            "birdsong-recognition                           2020-09-15 23:59:00  Research           $25,000        405           False  \n",
            "landmark-retrieval-2020                        2020-08-17 23:59:00  Research           $25,000        168           False  \n",
            "siim-isic-melanoma-classification              2020-08-17 23:59:00  Featured           $30,000       2191           False  \n",
            "global-wheat-detection                         2020-08-04 23:59:00  Research           $15,000       1796           False  \n",
            "open-images-object-detection-rvc-2020          2020-07-31 16:00:00  Playground       Knowledge         58           False  \n",
            "open-images-instance-segmentation-rvc-2020     2020-07-31 16:00:00  Playground       Knowledge         12           False  \n",
            "hashcode-photo-slideshow                       2020-07-27 23:59:00  Playground       Knowledge         61           False  \n",
            "prostate-cancer-grade-assessment               2020-07-22 23:59:00  Featured           $25,000        923           False  \n",
            "alaska2-image-steganalysis                     2020-07-20 23:59:00  Research           $25,000       1051           False  \n",
            "m5-forecasting-accuracy                        2020-06-30 23:59:00  Featured           $50,000       5558           False  \n",
            "m5-forecasting-uncertainty                     2020-06-30 23:59:00  Featured           $50,000        909           False  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zfyy1ugypVVd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "eadcd4d9-56fc-4444-fe53-3992be5e9fbf"
      },
      "source": [
        "!kaggle competitions download -c digit-recognizer"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.6 / client 1.5.4)\n",
            "Downloading test.csv.zip to /content\n",
            "  0% 0.00/6.09M [00:00<?, ?B/s]\n",
            "100% 6.09M/6.09M [00:00<00:00, 101MB/s]\n",
            "Downloading sample_submission.csv to /content\n",
            "  0% 0.00/235k [00:00<?, ?B/s]\n",
            "100% 235k/235k [00:00<00:00, 73.9MB/s]\n",
            "Downloading train.csv.zip to /content\n",
            " 87% 8.00M/9.16M [00:00<00:00, 81.9MB/s]\n",
            "100% 9.16M/9.16M [00:00<00:00, 80.4MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dUG_Y9AnpbpF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip -q \"../content/train.csv.zip\"\n",
        "!unzip -q \"../content/test.csv.zip\""
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "apU6GbFspl3V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = pd.read_csv(\"../content/train.csv\")\n",
        "test = pd.read_csv(\"../content/test.csv\")"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eWB4vhVDpxX5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "outputId": "69b80ed0-3495-4877-cd1a-c39eb1ef956c"
      },
      "source": [
        "train.head()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>pixel0</th>\n",
              "      <th>pixel1</th>\n",
              "      <th>pixel2</th>\n",
              "      <th>pixel3</th>\n",
              "      <th>pixel4</th>\n",
              "      <th>pixel5</th>\n",
              "      <th>pixel6</th>\n",
              "      <th>pixel7</th>\n",
              "      <th>pixel8</th>\n",
              "      <th>pixel9</th>\n",
              "      <th>pixel10</th>\n",
              "      <th>pixel11</th>\n",
              "      <th>pixel12</th>\n",
              "      <th>pixel13</th>\n",
              "      <th>pixel14</th>\n",
              "      <th>pixel15</th>\n",
              "      <th>pixel16</th>\n",
              "      <th>pixel17</th>\n",
              "      <th>pixel18</th>\n",
              "      <th>pixel19</th>\n",
              "      <th>pixel20</th>\n",
              "      <th>pixel21</th>\n",
              "      <th>pixel22</th>\n",
              "      <th>pixel23</th>\n",
              "      <th>pixel24</th>\n",
              "      <th>pixel25</th>\n",
              "      <th>pixel26</th>\n",
              "      <th>pixel27</th>\n",
              "      <th>pixel28</th>\n",
              "      <th>pixel29</th>\n",
              "      <th>pixel30</th>\n",
              "      <th>pixel31</th>\n",
              "      <th>pixel32</th>\n",
              "      <th>pixel33</th>\n",
              "      <th>pixel34</th>\n",
              "      <th>pixel35</th>\n",
              "      <th>pixel36</th>\n",
              "      <th>pixel37</th>\n",
              "      <th>pixel38</th>\n",
              "      <th>...</th>\n",
              "      <th>pixel744</th>\n",
              "      <th>pixel745</th>\n",
              "      <th>pixel746</th>\n",
              "      <th>pixel747</th>\n",
              "      <th>pixel748</th>\n",
              "      <th>pixel749</th>\n",
              "      <th>pixel750</th>\n",
              "      <th>pixel751</th>\n",
              "      <th>pixel752</th>\n",
              "      <th>pixel753</th>\n",
              "      <th>pixel754</th>\n",
              "      <th>pixel755</th>\n",
              "      <th>pixel756</th>\n",
              "      <th>pixel757</th>\n",
              "      <th>pixel758</th>\n",
              "      <th>pixel759</th>\n",
              "      <th>pixel760</th>\n",
              "      <th>pixel761</th>\n",
              "      <th>pixel762</th>\n",
              "      <th>pixel763</th>\n",
              "      <th>pixel764</th>\n",
              "      <th>pixel765</th>\n",
              "      <th>pixel766</th>\n",
              "      <th>pixel767</th>\n",
              "      <th>pixel768</th>\n",
              "      <th>pixel769</th>\n",
              "      <th>pixel770</th>\n",
              "      <th>pixel771</th>\n",
              "      <th>pixel772</th>\n",
              "      <th>pixel773</th>\n",
              "      <th>pixel774</th>\n",
              "      <th>pixel775</th>\n",
              "      <th>pixel776</th>\n",
              "      <th>pixel777</th>\n",
              "      <th>pixel778</th>\n",
              "      <th>pixel779</th>\n",
              "      <th>pixel780</th>\n",
              "      <th>pixel781</th>\n",
              "      <th>pixel782</th>\n",
              "      <th>pixel783</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 785 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   label  pixel0  pixel1  pixel2  ...  pixel780  pixel781  pixel782  pixel783\n",
              "0      1       0       0       0  ...         0         0         0         0\n",
              "1      0       0       0       0  ...         0         0         0         0\n",
              "2      1       0       0       0  ...         0         0         0         0\n",
              "3      4       0       0       0  ...         0         0         0         0\n",
              "4      0       0       0       0  ...         0         0         0         0\n",
              "\n",
              "[5 rows x 785 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zBKc34LDsjq4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "outputId": "28b80004-f2ca-4c6c-fc1d-96c3a4af07b8"
      },
      "source": [
        "y_train = train[\"label\"]\n",
        "\n",
        "# Drop 'label' column\n",
        "x_train = train.drop( labels=[\"label\"], axis=1 )\n",
        "\n",
        "# Free some space\n",
        "del train\n",
        "\n",
        "g = sns.countplot(y_train)\n",
        "\n",
        "y_train.value_counts()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    4684\n",
              "7    4401\n",
              "3    4351\n",
              "9    4188\n",
              "2    4177\n",
              "6    4137\n",
              "0    4132\n",
              "4    4072\n",
              "8    4063\n",
              "5    3795\n",
              "Name: label, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZMAAAEMCAYAAAABLFv3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAASFklEQVR4nO3de5DdZX3H8XeymwCionKLXIOXfKUMaoNWrKDVglorRdTWxmDA1NFgR614BS2iFkRuihJMRtSGS9MWFbTqjNZpUSOlVQrt4OUb6pBAgEAgKOGe7G7/+P0Wl0BMzj5nn5Oz+37N7Jw9z3MOzzdkcz77/C7PM21kZARJkkpM73UBkqT+Z5hIkooZJpKkYoaJJKmYYSJJKjbY6wJ6ISJ2AF4I3AYM9bgcSeoXA8DTgZ9k5kNjO6ZkmNAEyY96XYQk9anDgRVjG6ZqmNwGcOmllzJr1qxe1yJJfWHt2rXMnz8f2s/QsaZqmAwBzJo1i3322afXtUhSv3nM6QFPwEuSihkmkqRihokkqZhhIkkqZphIkooZJpKkYoaJJKmYYbIdGd60cVKNI2nqmKo3LW6Xpg/O4Joz3zbh4xzywQsnfAxJU4szE0lSMcNEklTMMJEkFTNMJEnFDBNJUjHDRJJUzDCRJBUzTCRJxQwTSVIxw0SSVMwwkSQVM0wk9YVNmzZNyrEmCxd6lNQXBgcHOeecc6qM9b73va/KOJOJMxNtd4YerrdEfs2xpMnMmYm2OwMzZ/CdBW+tMtZrLvpKlXGkyc6ZiSSpmGEiSSpmmEiSihkmkqRihokkqZhhIkl9ZuPQ8HY3lpcG61Ee3rSRmYMzJt1Y0mQyY2A6J17+gypjnXvMy7bpdYaJHmXm4AyO/8p7qoz19289r8o4Kje8aYjpgwOTbix1j2ECPLxxiJkz6vzw1hxL6pbpgwP8zwVXVhnree/8oyrjqLsME2DmjAHe/MFLq4z1D2fOrzKOJNVUPUwi4mPAqcDBmXl9RBwKLAV2AlYBx2bmHe1rx9UndcOmjUMMVppF1hxLmghVwyQi5gKHAqvb59OBS4DjM3NFRHwUOANYON6+mn8eTW6DMwY4/SNfrTLWyae9sco4Kjc8tJHpA3UuHKk5VqlqYRIROwCLgXnAlW3zIcCDmbmifb6EZpaxsKBPkibM9IEZ/PBbp1YZ66WvrTNON9S8z+QTwCWZuWpM2360sxSAzLwTmB4RTyvokyRVViVMIuLFwAuAC2qMJ0mqq9bM5GXAgcCNEbEK2Af4LvAsYP/RF0XEbsBwZq4HbhpnnySpsiphkplnZOZemTk7M2cDa4BXAWcBO0XEYe1LFwGXtd9fM84+SVJlPV2bKzOHgbcAX4iIG2hmMB8u6ZMk1deTmxbb2cno91cBB2/hdePqkyTV5arBkqRihokkqZhhIkkqZphIkooZJtJ2bNPGjZNyLE0+LkEvbccGZ8zg3JPeUWWsEz+1tMo4mpycmUiSihkmkqRihokkqZhhIkkqZphIkooZJpKkYoaJJKmYYSJJKmaYSJKKGSaSpGKGiSSpmGEiSSpmmEiSihkmkqRihokkqZhhIkkqZphIkooZJpKkYoaJJKmYYSJJKmaYSJKKGSaSpGKGiSSpmGEiSSpmmEiSihkmkqRihokkqZhhIkkqZphIkooN1hooIq4ADgCGgXuBd2XmdRExB1gG7ArcBSzIzBva94yrT5JUV82ZyXGZ+bzM/H3gbODLbfsSYHFmzgEWA0vHvGe8fZKkiqrNTDLzN2Oe7gIMR8QewFzgyLZ9OXB+ROwOTBtPX2aum9g/iSRpc1XPmUTEhRFxE3AacBywL3BLZg4BtI+3tu3j7ZMkVVY1TDLzbZm5H3AycFbNsSVJE6cnV3Nl5sXAy4E1wN4RMQDQPu4F3Nx+jadPklRZlTCJiCdGxL5jnh8FrAfuAK4D5rVd84BrM3NdZo6rb+L/NJKkzdU6Ab8zcFlE7AwM0QTJUZk5EhGLgGURcQpwN7BgzPvG2ydJqqhKmGTm7cChW+j7JfCibvZJkuryDnhJUjHDRJJUzDCRJBUzTCRJxbY5TCLi/VtoP7F75UiS+lEnM5NTttD+0W4UIknqX1u9NDgiXtF+OxARL6dZZHHUM4ANE1GYJKl/bMt9Jl9qH3fkt8vGA4wAa4F3dbsoSVJ/2WqYZOYBABFxUWZ6l7kk6TG2+Q74sUESEdM36xvuZlGSpP6yzWESEXNpdjR8Ls0hL2jOn4wAA90vTZLULzpZm2sZ8C/AQuD+iSlHktSPOgmT/YGPZObIRBUjSepPndxncjnwyokqRJLUvzqZmewIXB4RK2guCX6EV3lJ0tTWSZj8vP2SJOlROrk0+OMTWYgkqX91cmnwK7bUl5n/1p1yJEn9qJPDXF/a7PnuwExgDc0aXZKkKaqTw1wHjH0eEQM0Kwa70KMkTXHj3hwrM4eA04APdq8cSVI/Kt1p8UjAdbkkaYrr5AT8zTTrcI16As29J+/sdlGSpP7SyQn4Yzd7fh+wMjPv6WI9kqQ+1MkJ+B/AI8vP7wnc7tLzkiTo4JxJRDwpIi4CHgBuAR6IiGURscuEVSdJ6gudnID/PLAzcDCwU/v4BOBzE1CXJKmPdHLO5NXAMzJzdC+TlRHxVuBX3S9LktRPOpmZPEhz1/tYuwEPda8cSVI/6mRmciHwrxFxLrCaZrOs9wJfnIjCJEn9o5MwOY3mxPt8YC/gVuDMzNx8zS5J0hTTyWGu84DMzCMy8/cy8wjgFxHx2QmqTZLUJzoJk3nATzdruwZ4c/fKkST1o07CZAQY2KxtoMP/hiRpEuokCH4EfLK9A370TvhT23ZJ0hTWyQn49wDfAm6LiNXAfsBtwFFbe2NE7ApcDDwTeBi4AXhHZq6LiEOBpTQ3Qq4Cjs3MO9r3jatPklTXNs9MMnMNMBc4GjgLeB1wSNu+NSM0V35FZh5Mc6PjGe3s5hLgrzNzDvBD4Ax4ZObTcZ8kqb5OZia0Czte3X518r71wJVjmq4GTgAOAR7MzBVt+xKaWcbCgj5JUmXVT563s4oTgG/SHCpbPdqXmXcC0yPiaQV9kqTKenEl1ueBe4HzezC2JGkCVA2TiDgbeDbwpvaQ2U00y7KM9u8GDLeHxcbbJ0mqrFqYRMTpNOc6XpeZo4tDXgPsFBGHtc8XAZcV9kmSKuvoBPx4RcRBwEnASuCqiAC4MTOPiYi3AEsjYkfaS3yhOdk/nj5JUn1VwiQzfwZM20LfVTQbbXWtT5JUl0uhSJKKGSaSpGKGiSSpmGEiSSpmmEiSihkmkqRihokkqZhhIkkqZphIkooZJpKkYoaJJKmYYSJJKmaYSJKKGSaSpGKGiSSpmGEiSSpmmEiSihkmkqRihokkqZhhIkkqZphIkooZJpKkYoaJJKmYYSJJKmaYSJKKGSaSpGKGiSSpmGEiSSpmmEiSihkmkqRihokkqZhhIkkqZphIkooZJpKkYoaJJKnYYI1BIuJs4A3AbODgzLy+bZ8DLAN2Be4CFmTmDSV9kqT6as1MrgBeCqzerH0JsDgz5wCLgaVd6JMkVVZlZpKZKwAi4pG2iNgDmAsc2TYtB86PiN2BaePpy8x1E/xHkSQ9jl6eM9kXuCUzhwDax1vb9vH2SZJ6wBPwkqRivQyTm4G9I2IAoH3cq20fb58kqQd6FiaZeQdwHTCvbZoHXJuZ68bbV696SdJYtS4N/hzwemAW8P2IuCszDwIWAcsi4hTgbmDBmLeNt0+SVFmtq7neDbz7cdp/CbxoC+8ZV58kqT5PwEuSihkmkqRihokkqZhhIkkqZphIkooZJpKkYoaJJKmYYSJJKmaYSJKKGSaSpGKGiSSpmGEiSSpmmEiSihkmkqRihokkqZhhIkkqZphIkooZJpKkYoaJJKmYYSJJKmaYSJKKGSaSpGKGiSSpmGEiSSpmmEiSihkmkqRihokkqZhhIkkqZphIkooZJpKkYoaJJKmYYSJJKmaYSJKKGSaSpGKGiSSpmGEiSSo22OsCSkTEHGAZsCtwF7AgM2/obVWSNPX0+8xkCbA4M+cAi4GlPa5Hkqakvp2ZRMQewFzgyLZpOXB+ROyemeu28vYBgLVr1z7S8ND9v56IMh9jzZo1v7N/3YYHe17Dg7++f8Jr2Fod6x+a+P8PW6sB4N777u55HRvue6DnNQDccc+dPa9jw4YNPa8B4M719/a8jvvX1//7GPOZObD566aNjIxUKajbIuIQ4KLMPGhM28+BYzPzv7fy3sOAH01wiZI0WR2emSvGNvTtzKTQT4DDgduAoR7XIkn9YgB4Os1n6KP088xkD2AlsGtmDkXEAM1J+Gdvw2EuSVIX9e0J+My8A7gOmNc2zQOuNUgkqb6+nZkARMRzaC4NfipwN82lwdnbqiRp6unrMJEkbR/69jCXJGn7YZhIkooZJpKkYoaJJKnYVL1pscj2sMBkRJwNvAGYDRycmdfXHL+tYVfgYuCZwMPADcA7al+eHRFXAAcAw8C9wLsy87qaNYyp5WPAqfTu72QV8GD7BfChzPxuD+rYEfgMcERby39k5tsrjj8buGJM01OAJ2fm02rVMKaW1wKfBKa1Xx/PzK9XruFP2xpmAOuB4zPzxm6OYZiMz+gCk5dExLE0C0y+onINVwDn0dtlYUaAMzPzSoCIOAs4A/irynUcl5m/aWs4GvgyzbptVUXEXOBQYHXtsTfzxl4E2WbOpAmROZk5EhF71hw8M1cBzx99HhGfpQefdxExjeYXrsMz8/qIeC7w44i4IjOHK9XwVJpffv8wM1e2n1lfAF7dzXE8zNWhMQtMLm+blgNzI2L3mnVk5orMvLnmmI9Tw/rRIGldDezfgzp+M+bpLjQzlKoiYgealatPqD329iYinggsAP42M0cAMvP2HtYzE5hP80tGLwzT/FxCM0O6rVaQtJ4F3J6ZK9vn3wFeFRG7dXMQw6Rz+wK3ZOYQQPt4a9s+ZUXEdJoP0m/2aPwLI+Im4DTguB6U8AngkvY34l67NCL+NyIuiIin9GD8Z9Ic/v1YRPw0Iq5sF1ftlT+j+Tf7OxeAnQhtmP4F8I2IWE1zRGFB5TJWArMi4oXt8/nt437dHMQwUbd8nuZ8xfm9GDwz35aZ+wEnA2fVHDsiXgy8ALig5rhbcHhmPg94Ic3x+V78fQwAz6BZ3ugFwIeAr0fEk3tQC8BCejQriYhB4CTg6MzcHzgK+Od29lZFO3N/E/CZiPgpsAfwa2BTN8cxTDp3M7B3u7Ak7eNebfuU1F4M8GzgTZWn74+RmRcDL28vDqjlZcCBwI3tCfB9gO9GxCsr1gDA6KHPzHyIJtxeUrsG4CaaD6rlbS3/CdwJzKldSETsTfP3c2ntsVvPB/bKzB8DtI/30fy8VJOZ38/Mw9pwPx/YCfhVN8cwTDrkApOPFhGnA4cAr2s/wGqP/8SI2HfM86NorlZZX6uGzDwjM/fKzNmZORtYA7wqM79XqwaAiNg5InZpv58G/CXNz2pVmXkn8O+0G9e1Vz/uAfxf7VpoDnl+OzPv6sHY0Pws7BMRARARBwJ70uUP8q2JiFnt43TgdGBJZt7XzTG8mmt8FgHLIuIU2gUmaxcQEZ8DXg/MAr4fEXeN3SisUg0H0UzhVwJXtf9ebszMYyqWsTNwWUTsTLM3zXrgqNETv1PMnsDX2tnyAPBz4J09qmUR8OWIOAfYCLwlM+tsZ/poxwPv7sG4AGTm2og4AfhqRIzO2hdmZrVfdlp/FxEvAWYC3wM+3O0BXOhRklTMw1ySpGKGiSSpmGEiSSpmmEiSihkmkqRihok0gSJiVUQcsQ2vG4mIZ41zjHG/V+oWw0SSVMwwkSQV8w54qYKI+AOa/WcOBB4AvgacmJkPj3nZayLib4AnA1+h2dhquH3/QuADNCse/Bfw9szs9b4p0iOcmUh1DAHvBXYDXgz8MY9d6uQYmtWH5wJH06x2O7rh18k0y+fsTrMh2nKk7YhhIlWQmddk5tWZuand82QpzWq2Y3263XDsJuCz/HYx0UXApzLzF5m5iWahvudHRPWNyKQt8TCXVEG7cu65NDOPJ9D827tms5eN3cZgNc3WBtDsXnleu2jiqGnA3vR+i2AJMEykWr4AXAvMy8wN7bmRN272mn2Bn7Xf70ezgyc0IXNaZvZqTw5pqzzMJdXxJOAe4N6IeA6Pv1f8ByLiqe3+LO8B/qltXwKc1C75T0TsEhF/XqNoaVsZJlId7wfeDGwAvshvg2Ksb9Ac+roO+DbwJYDMvBz4NPCPEXEPcD3wJxVqlraZ+5lIkoo5M5EkFTNMJEnFDBNJUjHDRJJUzDCRJBUzTCRJxQwTSVIxw0SSVMwwkSQV+3+06vcxt9JDBwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fSA8_xnUuFSE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b728d259-6d8f-4c85-befb-b565d1ccbca7"
      },
      "source": [
        "x_train.shape"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(42000, 784)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2E6l9uojuHzp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b322a62c-ac77-4d72-fd4d-a736e6596475"
      },
      "source": [
        "y_train.shape"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(42000,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fnhnLROcuLmO",
        "colab_type": "text"
      },
      "source": [
        "### Check for null or missing values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FcqDy-RHuPo6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "096b3db9-479b-49fe-c5cf-c74a9462fd90"
      },
      "source": [
        "# Check data\n",
        "x_train.isnull().any().describe()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count       784\n",
              "unique        1\n",
              "top       False\n",
              "freq        784\n",
              "dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0sxyaHxquWcs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "209829d1-79fb-4f50-8bc5-b14cb4446527"
      },
      "source": [
        "test.isnull().any().describe()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count       784\n",
              "unique        1\n",
              "top       False\n",
              "freq        784\n",
              "dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8roSlH_ueoj",
        "colab_type": "text"
      },
      "source": [
        "No missing value in train and test datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pxi0MP00uiwt",
        "colab_type": "text"
      },
      "source": [
        "### Normalisation\n",
        "We perform a grayscale normalisation to reduce the effect of illumination differences.\n",
        "\n",
        "Moreover the CNN faster on [0...1] data than on [0...255]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aySQC3__u9ew",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Normalise the data\n",
        "x_train = x_train / 255\n",
        "test = test / 255"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "biwYEN9BvdwS",
        "colab_type": "text"
      },
      "source": [
        "### Reshape"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QW1kwOV_vgda",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Reshape image in 3 dimensions (height = 28px, widht=28px, canal =1)\n",
        "x_train = x_train.values.reshape(-1, 28, 28, 1)\n",
        "test = test.values.reshape(-1, 28, 28, 1)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1s_nl5awEEL",
        "colab_type": "text"
      },
      "source": [
        "Train and test images (28px x 28px) has been stocked into pandas.DataFrame as 1D vectors of 784 values.\n",
        "\n",
        "We reshape all datga to 28x28x1 3D matrices.\n",
        "\n",
        "Keras requries an extra dimension in the end which correspond to channels.\n",
        "\n",
        "MNIST images are gray scaled so it use only 1 channel.\n",
        "\n",
        "FOr RGB images, there are 3 channels, we sould have reshaped 784px vectors to 28x28x3 3D matrices."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HaX4-cd0wt0q",
        "colab_type": "text"
      },
      "source": [
        "### Label Encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFynnsWewwyI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Encode labels to one hot vectors (eg, 2 -> [0,0,1,0,0,0,0,0,0])\n",
        "y_train = to_categorical( y_train, num_classes=10 )"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_9nZU0IyCek",
        "colab_type": "text"
      },
      "source": [
        "### Split training and validation set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-kxhH41iyH9m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set random seed\n",
        "random_seed = 2"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJi4gwdbyNB9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Split train and validation set for fitting\n",
        "x_train, x_val, y_train, y_val = train_test_split( x_train, y_train, test_size=0.1, random_state=random_seed )"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jGmBaP9UymRf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        },
        "outputId": "7e7d85d2-c2b7-47f0-f15c-50dd4d1f5a2c"
      },
      "source": [
        "# Some examples\n",
        "g = plt.imshow( x_train[0][:,:,0] )"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD7CAYAAAClmULcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAANq0lEQVR4nO3df6zV9X3H8ef1YiyjxgnVidQfm5X3FJlWJG2kuMzFP5bFzIoRsUqWLItaErO4OFOrZtlSZ8B2maKDrH+UKmrTkBj7hzHtJlVqTAmRTMS+oZ1Yhqwi16ayFhS4++N+WW8p93sO93zPPefyeT6Sm3P5vPl+z9uvefE95/v5/hgYHh5GUnlO6nUDknrD8EuFMvxSoQy/VCjDLxVqSq/eOCJOAeYDu4FDvepDOoENAjOBjZl54Ohix+GPiNnAGmAGsBdYmpnb21h0PvByp+8vqaWFwIajB5vY868CHsvMJyPiFmA1cHUby+0G+O9d/8vBQ55rIDVtyuAAn5w1Daqs/Va9k5VHxJnA5cA11dDTwMqIOCMz97RY/BDAwUPDHDxo+KUuOubX6k4P+J0D7MrMQwDV6zvVuKQ+5tF+qVCdhn8nMCsiBgGq17OrcUl9rKPwZ+a7wGZgSTW0BHitje/7knqsiaP9twNrIuIB4H1gaQPrlNRlHYc/M38EfKaBXiRNIA/4SYUy/FKhDL9UKMMvFcrwS4Uy/FKhDL9UKMMvFcrwS4Uy/FKhDL9UKMMvFcrwS4Uy/FKhDL9UKMMvFcrwS4Uy/FKhDL9UKMMvFcrwS4Uy/FKhDL9UKMMvFcrwS4Uy/FKhDL9UKMMvFcrwS4Vq4hHdKsiimfNr6w+e+qva+nnr/3XM2kfPfLV22Y82baut//V/TKutr9u9sbZemo7DHxE7gP3VD8A9mflCp+uV1F1N7flvyMwtDa1L0gTwO79UqKb2/GsjYgDYANybmT9vaL2SuqSJPf/CzLwUmA8MACsbWKekLus4/Jm5s3o9ADwOLOh0nZK6r6PwR8S0iDit+n0AuAnY3ERjkrqr0+/8vwesi4hBYBDYCnyx467UM9+ZflVtffapQ7X1ez+YWltfd/bC4+7piH+c+Se19TXf/NPa+oPLxj4HIbaVN1nVUfgz87+ATzfUi6QJ5FSfVCjDLxXK8EuFMvxSoQy/VCgv6T0BXTT9nDFrP/izU2uX/cHz9evu5ZTY/btfrK0/tfjHtfW6//ah+RfXLjt97dba+mTknl8qlOGXCmX4pUIZfqlQhl8qlOGXCmX4pUI5z38C+uF988asvbOqfi782qGXmm5nwrw5tLO2vuD5sc9/2Pit22qXvej51R29dz9yzy8VyvBLhTL8UqEMv1Qowy8VyvBLhTL8UqGc55+EWj0me2DuFWPWYtuzTbczadx8yqfGrE2Z88e1y745dF/T7fSce36pUIZfKpThlwpl+KVCGX6pUIZfKpThlwrlPP8ktHbT12rrX5h31wR10l9anf9w96Z/GPe697/zcm39l3fX3w+gH+/73zL8EfEwsAg4H5ibmVuq8dnAGmAGsBdYmpnbu9eqpCa187H/WeAq4O2jxlcBj2XmbOAxoP5WJ5L6SsvwZ+aGzPyNexRFxJnA5cDT1dDTwOURcUbzLUrqhvEe8DsH2JWZhwCq13eqcUmTgEf7pUKNN/w7gVkRMQhQvZ5djUuaBMYV/sx8F9gMLKmGlgCvZeaephqT1F3tTPU9AlwPnAV8LyL2ZuYc4HZgTUQ8ALwPLO1qpwVpNV/dyrrdGxvqpL+02i6tzn/oRKt5/AXPf9C19+6WluHPzDuBO48x/iPgM91oSlL3ecBPKpThlwpl+KVCGX6pUIZfKpSX9Pahy/h4r1vomrrpum/cPat22ZNv+tuO3vujZ746Zu0vV+yqXXbd7v67JLdT7vmlQhl+qVCGXyqU4ZcKZfilQhl+qVCGXyqU8/x9aDP7Olq+bi6908t9L5pef6e2H943r7ZeN1d/8I3v1y773Uu+XFv/O96qrb855L1mRnPPLxXK8EuFMvxSoQy/VCjDLxXK8EuFMvxSoZzn70Ot5uL/rcVtpNduGvuZqVsvubl22eX8fm39mi1fqa23smLeA2PW7t/9Ykfr1vFxzy8VyvBLhTL8UqEMv1Qowy8VyvBLhTL8UqGc55+Epq+tv4f8/hVj117b8lRH7+019SeOtsIfEQ8Di4DzgbmZuaUa3wHsr34A7snMFxrvUlLj2t3zPwv8C/DyMWo3HPnHQNLk0Vb4M3MDQER0txtJE6aJ7/xrI2IA2ADcm5k/b2Cdkrqs06P9CzPzUmA+MACs7LwlSROho/Bn5s7q9QDwOLCgiaYkdd+4wx8R0yLitOr3AeAmYHNTjUnqrnan+h4BrgfOAr4XEXuBa4F1ETEIDAJbgS92q1H92nemXzXuZeueUQ/199UH5/FPJO0e7b8TuPMYpU83246kieLpvVKhDL9UKMMvFcrwS4Uy/FKhvKS3D33wtetq6x9t2lZb/8K8u8astbot+KIVu2rrG79Vf9vwXcueqa3HNq8B6xfu+aVCGX6pUIZfKpThlwpl+KVCGX6pUIZfKpTz/F1w0fRzaus3n/Kp2nqrefxWt+7uRKvzALYu/p/aestbg5+98HhbUpe455cKZfilQhl+qVCGXyqU4ZcKZfilQhl+qVDO83dBq2ve/3npv9fWuzmP3ylvzX3icM8vFcrwS4Uy/FKhDL9UKMMvFcrwS4Uy/FKhnOcfp04ek33/7hcb7GRitbpXgSaPluGPiBnAE8AFwIfAduC2zNwTEZ8FVgNTgR3ALZn5bvfaldSUdj72DwPLMzMycy7wE+ChiDgJeBJYlpmzgZeAh7rXqqQmtQx/Zg5l5vpRQ68C5wHzgP2ZuaEaXwXc2HiHkrriuA74VXv7O4DngHOBt4/UMvM94KSImN5oh5K64niP9j8K7ANWdqEXSROo7fBHxMPAhcDizDwM/JSRj/9H6p8ADmfmUONdSmpcW1N9EfEgI9/x/zwzD1TDm4CpEfG56nv/7cC3u9Pm5NLqkt1+1moqr9Xlygff+H6T7aiL2pnqmwN8CdgGvBIRAG9l5ucj4lZgdUR8jGqqr4u9SmpQy/Bn5hvAwBi1V4C5TTclqfs8vVcqlOGXCmX4pUIZfqlQhl8qlJf0jtM1W74yZu0b8+6awE6Oz6KZ82vra775Fx2tf/7i1R0tr4njnl8qlOGXCmX4pUIZfqlQhl8qlOGXCmX4pUI5zz9OK+Y9MGat1Vz5ZUs/3tF7L7v6Z7X131kx/rn2717y5dr6tUMvjXvd6i/u+aVCGX6pUIZfKpThlwpl+KVCGX6pUIZfKpTz/ONU95jtG5ftqV122dWHa+ut5ul/eXf9vfPrzkF46sCPa5d9c2hnbV0nDvf8UqEMv1Qowy8VyvBLhTL8UqEMv1Qowy8Vqp1HdM8AngAuAD4EtgO3ZeaeiBgGXgeOTFzfmpmvd6vZySK2ban/C9tarGDtwg472Nrh8ipBOyf5DAPLM3M9QESsAB4C/qqqX5mZ+7rTnqRuaRn+zBwC1o8aehW4o1sNSZoYx3V6b0ScxEjwnxs1vD4ipgDPA3+fmQca7E9SlxzvAb9HgX3AyurP52bmFcBVwMXA/Q32JqmL2g5/RDwMXAgszszDAJm5s3r9BfB1YEE3mpTUvLbCHxEPAvOA6458rI+I0yNiavX7FOAGYHO3GpXUrHam+uYAX2JkguqViAB4C1gOrK6m+04GXsGP/dKk0c7R/jeAgTHKf9RsO5Imimf4SYUy/FKhDL9UKMMvFcrwS4Uy/FKhDL9UKMMvFcrwS4Uy/FKhDL9UKMMvFcrwS4Xq5VN6BwGmDI51waCkTozK1uAx6xPXym+ZCfDJWdN62IJUhJnAT44e7GX4NwILgd3AoR72IZ2oBhkJ/sZjFQeGh4cnth1JfcEDflKhDL9UKMMvFcrwS4Uy/FKhDL9UKMMvFaqXJ/n8v4iYDawBZgB7gaWZub23XY2IiB3A/uoH4J7MfKEHfTwMLALOB+Zm5pZqvOfbrqa3HfR420XEDOAJ4ALgQ2A7cFtm7omIzwKrganADuCWzHy3T3obBl4HDld//dbMfL3J9++XPf8q4LHMnA08xsj/kH5yQ2ZeVv1MePArzzLyNOS3jxrvh203Vm/Q+203DCzPzMjMuYyc5vpQ9bj5J4Fl1bZ7CXioH3obVb9y1LZrNPjQB+GPiDOBy4Gnq6Gngcsj4ozeddV/MnPDkaciH9Ev2+5YvfWLzBzKzPWjhl4FzmPkwbP7M3NDNb4KuLFPepsQPQ8/cA6wKzMPAVSv71Tj/WJtRPxnRDweEb/b62ZGcdsdh2pvfwfwHHAuoz6pZOZ7wEkRMb0PejtifURsjoh/iohTmn7Pfgh/v1uYmZcC8xl5YOnKHvczmfTbtnsU2NcHfRzL0b2dm5lXMPJ16mK68ATsfgj/TmBWRAwCVK9nV+M9d+TjbGYeAB4HFvS2o9/gtmtTdVDyQmBxZh4Gfsqoj9gR8QngcGYO9UFvo7fdL4Cv04Vt1/PwV0dXNwNLqqElwGuZuad3XY2IiGkRcVr1+wBwEyO99gW3Xdu9PMjId/zrqn+IADYBUyPic9Wfbwe+3Q+9RcTpETG1+n0KcANd2HZ9cUlvRPwhI9NVpwPvMzJdlb3tCiLiD4B1jFwXPQhsBe7MzN096OUR4HrgLOA9YG9mzumHbXes3oBr6YNtFxFzgC3ANuBX1fBbmfn5iLiSkdmRj/Hrqb6f9bo3YHnV1zBwMvAK8DeZua/J9++L8EuaeD3/2C+pNwy/VCjDLxXK8EuFMvxSoQy/VCjDLxXK8EuF+j+nwNO799hx2AAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pL0wEETf-JGh",
        "colab_type": "text"
      },
      "source": [
        "## 2. CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stvFIIRu-ZYl",
        "colab_type": "text"
      },
      "source": [
        "I used the Keras Sequential API, where you have to add one layer at a time, starting from the input.\n",
        "\n",
        "The first is the convolutional (Conv2D) layer. It is like a set of learnable filters. I choosed to set 32 filters for the first 2 conv2D layers and 64 filters for the two last ones. Each filter transforms a part of the image (defined by the kernel size) using the kernel filter. The kernel filter matrix is applied on the whole image. Filters can be seen as a transformation of the image.\n",
        "\n",
        "The CNN can isolate features that are useful everywhere from these transformed images (feature maps).\n",
        "\n",
        "The second important layer in CNN is the pooling (MaxPool2D) layer. This layer simply acts as a downsampling filter. It looks at the 2 neighboring pixels and picks the maximal value. These are used to reduce computational cost, and to some extent also reduce overfitting. We have to choose the pooling size (i.e the area size pooled each time) more the pooling dimension is high, more the downsampling is important.\n",
        "\n",
        "Combining convolutional and pooling layers, CNN are able to combine local features and learn more global features of the image.\n",
        "\n",
        "Dropout is a regularization method, where a proportion of nodes in the layer are randomly ignored (setting their wieghts to zero) for each training sample. This drops randomly a propotion of the network and forces the network to learn features in a distributed way. This technique also improves generalization and reduces the overfitting.\n",
        "\n",
        "'relu' is the rectifier (activation function max(0,x). The rectifier activation function is used to add non linearity to the network.\n",
        "\n",
        "The Flatten layer is use to convert the final feature maps into a one single 1D vector. This flattening step is needed so that you can make use of fully connected layers after some convolutional/maxpool layers. It combines all the found local features of the previous convolutional layers.\n",
        "\n",
        "In the end i used the features in two fully-connected (Dense) layers which is just artificial an neural networks (ANN) classifier. In the last layer(Dense(10,activation=\"softmax\")) the net outputs distribution of probability of each class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YaOs3WXcK8gZ",
        "colab_type": "text"
      },
      "source": [
        "### Define CNN model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2YYVRFvr-IsM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set the CNN model\n",
        "# CNN architecture is in -> [ [Conv2D->relu]*2 -> MaxPool2D -> Dropout ] *2 -> Flatten -> Dense -> Dropout -> Out\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(\n",
        "    Conv2D(\n",
        "        filters = 32,\n",
        "        kernel_size = (5,5),\n",
        "        padding = 'Same',\n",
        "        activation = 'relu',\n",
        "        input_shape = (28,28,1)\n",
        "    )\n",
        ")\n",
        "model.add(\n",
        "    Conv2D(\n",
        "        filters = 32,\n",
        "        kernel_size = (5,5),\n",
        "        padding = 'Same',\n",
        "        activation = 'relu'\n",
        "    )\n",
        ")\n",
        "model.add(\n",
        "    MaxPool2D( pool_size=(2,2) )\n",
        ")\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "\n",
        "model.add(\n",
        "    Conv2D(\n",
        "        filters = 64,\n",
        "        kernel_size = (3,3),\n",
        "        padding = 'Same',\n",
        "        activation = 'relu',\n",
        "    )\n",
        ")\n",
        "model.add(\n",
        "    Conv2D(\n",
        "        filters = 64,\n",
        "        kernel_size = (3,3),\n",
        "        padding = 'Same',\n",
        "        activation = 'relu',\n",
        "    )\n",
        ")\n",
        "model.add(\n",
        "    MaxPool2D( \n",
        "        pool_size=(2,2), \n",
        "        strides=(2,2) )\n",
        ")\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(256, activation=\"relu\"))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(256, activation=\"softmax\"))"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2nyfXMlFARho",
        "colab_type": "text"
      },
      "source": [
        "### Set optimizer and annealer\n",
        "\n",
        "Once our layers are added to the model, we need to set up a score function, a loss function and an optimisation algorithm.\n",
        "\n",
        "We define the loss function to measure how poorly our model performs on images with known labels. It is the error rate between the oberved labels and the predicted ones. We use a specific form for categorical classifications (>2 classes) called the \"categorical_crossentropy\".\n",
        "\n",
        "The most important function is the optimizer. This function will iteratively improve parameters (filters kernel values, weights and bias of neurons ...) in order to minimise the loss.\n",
        "\n",
        "I choosed RMSprop (with default values), it is a very effective optimizer. The RMSProp update adjusts the Adagrad method in a very simple way in an attempt to reduce its aggressive, monotonically decreasing learning rate. We could also have used Stochastic Gradient Descent ('sgd') optimizer, but it is slower than RMSprop.\n",
        "\n",
        "The metric function \"accuracy\" is used is to evaluate the performance our model. This metric function is similar to the loss function, except that the results from the metric evaluation are not used when training the model (only for evaluation)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ceqE-ZX6Jn83",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define Optimizer\n",
        "optimizer = RMSprop(\n",
        "    lr=0.001,\n",
        "    rho=0.9,\n",
        "    epsilon=1e-08,\n",
        "    decay=0.0\n",
        ")"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJ0TlYZjKIEY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Compile model\n",
        "model.compile(\n",
        "    optimizer=optimizer,\n",
        "    loss=\"categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"]\n",
        ")"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFe6yj9fKXJ9",
        "colab_type": "text"
      },
      "source": [
        "In order to make the optimizer converge faster and closest to the global minimum of the loss function, i used an annealing method of the learning rate (LR).\n",
        "\n",
        "The LR is the step by which the optimizer walks through the 'loss landscape'. The higher LR, the bigger are the steps and the quicker is the convergence. However the sampling is very poor with an high LR and the optimizer could probably fall into a local minima.\n",
        "\n",
        "Its better to have a decreasing learning rate during the training to reach efficiently the global minimum of the loss function.\n",
        "\n",
        "To keep the advantage of the fast computation time with a high LR, i decreased the LR dynamically every X steps (epochs) depending if it is necessary (when accuracy is not improved).\n",
        "\n",
        "With the ReduceLROnPlateau function from Keras.callbacks, i choose to reduce the LR by half if the accuracy is not improved after 3 epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZYd6J0sKa_I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set a learning rate annealer\n",
        "learning_rate_reduction = ReduceLROnPlateau(\n",
        "    monitor='val_acc',\n",
        "    patience=3,\n",
        "    verbose=1,\n",
        "    factor=0.5,\n",
        "    min_lr=0.00001\n",
        ")"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HkxP02SUKvSu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs = 3 #Turn epochs to 30 to get 0.9907 accuracy\n",
        "batch_size = 86"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBVZe1I6TBX8",
        "colab_type": "text"
      },
      "source": [
        "### Data augmentation\n",
        "In order to avoid overfitting problem, we need to expand artificially our handwritten digit dataset. We can make your existing dataset even larger. The idea is to alter the training data with small transformations to reproduce the variations occuring when someone is writing a digit.\n",
        "\n",
        "For example, the number is not centered The scale is not the same (some who write with big/small numbers) The image is rotated...\n",
        "\n",
        "Approaches that alter the training data in ways that change the array representation while keeping the label the same are known as data augmentation techniques. Some popular augmentations people use are grayscales, horizontal flips, vertical flips, random crops, color jitters, translations, rotations, and much more.\n",
        "\n",
        "By applying just a couple of these transformations to our training data, we can easily double or triple the number of training examples and create a very robust model.\n",
        "\n",
        "The improvement is important :\n",
        "\n",
        "Without data augmentation i obtained an accuracy of 98.114%\n",
        "With data augmentation i achieved 99.67% of accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXdML1qiTILM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Without data augmentation i obtained an accuracy of 0.98114\n",
        "#history = model.fit(X_train, Y_train, batch_size = batch_size, epochs = epochs, \n",
        "#          validation_data = (X_val, Y_val), verbose = 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BGZlrcrGTKNc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# With data augmentation to prevent overfitting (accuracy 0.99286)\n",
        "\n",
        "datagen = ImageDataGenerator(\n",
        "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
        "        samplewise_center=False,  # set each sample mean to 0\n",
        "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
        "        samplewise_std_normalization=False,  # divide each input by its std\n",
        "        zca_whitening=False,  # apply ZCA whitening\n",
        "        rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "        zoom_range = 0.1, # Randomly zoom image \n",
        "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
        "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
        "        horizontal_flip=False,  # randomly flip images\n",
        "        vertical_flip=False\n",
        "        )  # randomly flip images\n",
        "\n",
        "\n",
        "datagen.fit(x_train)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tILXMuy4TP7U",
        "colab_type": "text"
      },
      "source": [
        "For the data augmentation, i choosed to :\n",
        "\n",
        "Randomly rotate some training images by 10 degrees\n",
        "Randomly Zoom by 10% some training images\n",
        "Randomly shift images horizontally by 10% of the width\n",
        "Randomly shift images vertically by 10% of the height\n",
        "I did not apply a vertical_flip nor horizontal_flip since it could have lead to misclassify symetrical numbers such as 6 and 9.\n",
        "\n",
        "Once our model is ready, we fit the training dataset ."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uw-wpsBWTRjP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "2373cadd-02ab-4e53-8f63-e3178872c07b"
      },
      "source": [
        "# Fit the model\n",
        "history = model.fit_generator(\n",
        "    datagen.flow(\n",
        "        x_train,\n",
        "        y_train,\n",
        "        batch_size=batch_size\n",
        "    ),\n",
        "    epochs = epochs, \n",
        "    validation_data = (x_val, y_val),\n",
        "    verbose = 2,\n",
        "    steps_per_epoch=x_train.shape[0], # batch size\n",
        "    callbacks=[learning_rate_reduction]\n",
        ")\n"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-cc298c9b4abc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# batch size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlearning_rate_reduction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m )\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1730\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1731\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1732\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    148\u001b[0m                                      str(validation_data))\n\u001b[1;32m    149\u001b[0m                 val_x, val_y, val_sample_weights = model._standardize_user_data(\n\u001b[0;32m--> 150\u001b[0;31m                     val_x, val_y, val_sample_weight)\n\u001b[0m\u001b[1;32m    151\u001b[0m                 \u001b[0mval_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_x\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mval_y\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mval_sample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m                 if model.uses_learning_phase and not isinstance(K.learning_phase(),\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    619\u001b[0m                 \u001b[0mfeed_output_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    620\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 621\u001b[0;31m                 exception_prefix='target')\n\u001b[0m\u001b[1;32m    622\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    623\u001b[0m             \u001b[0;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    143\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    146\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Error when checking target: expected dense_2 to have shape (256,) but got array with shape (10,)"
          ]
        }
      ]
    }
  ]
}